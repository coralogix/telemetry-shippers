global:
  domain: ""
  clusterName: ""
  defaultApplicationName: "default"
  defaultSubsystemName: "nodes"
  fullnameOverride: otel-coralogix

  # Old endpoint based configuration,
  # please use domain instead.
  traces:
    endpoint: ""
  metrics:
    endpoint: ""
  logs:
    endpoint: ""

# set distribution to openshift for openshift clusters
distribution: ""
opentelemetry-collector:
  mode: daemonset
  serviceAccount:
    # Specifies whether a service account should be created
    create: true
    # Annotations to add to the service account
    annotations: {}
    # The name of the service account to use.
    # If not set and create is true, a name is generated using the fullname template
    name: ""
  hostNetwork: true
  dnsPolicy: "ClusterFirstWithHostNet"
  fullnameOverride: "otel-coralogix"
  clusterRole:
    name: "otel-coralogix"
    clusterRoleBinding:
      name: "otel-coralogix"
  presets:
    logsCollection:
      enabled: true
      storeCheckpoints: true
      maxRecombineLogSize: 1048576
      extraFilelogOperators: []
#     - type: recombine
#       combine_field: body
#       source_identifier: attributes["log.file.path"]
#       is_first_entry: body matches "^(YOUR-LOGS-REGEX)"
    kubernetesAttributes:
      enabled: true
    hostMetrics:
      enabled: true
    kubeletMetrics:
      enabled: true
    metadata:
      enabled: true
      clusterName: "{{.Values.global.clusterName}}"
      integrationName: "coralogix-otel-agent-helm"

  extraEnvs:
  - name: CORALOGIX_PRIVATE_KEY
    valueFrom:
      secretKeyRef:
        name: coralogix-keys
        key: PRIVATE_KEY
  - name: OTEL_RESOURCE_ATTRIBUTES
    value: "k8s.node.name=$(K8S_NODE_NAME)"
  - name: KUBE_NODE_NAME
    valueFrom:
      fieldRef:
        apiVersion: v1
        fieldPath: spec.nodeName
  config:
    extensions:
      zpages:
        endpoint: localhost:55679
      pprof:
        endpoint: localhost:1777
    exporters:
      coralogix:
        timeout: "30s"
        private_key: "${CORALOGIX_PRIVATE_KEY}"
        domain: "{{.Values.global.domain}}"
        traces:
          endpoint: "{{ .Values.global.traces.endpoint }}"
        metrics:
          endpoint: "{{ .Values.global.metrics.endpoint }}"
        logs:
          endpoint: "{{ .Values.global.logs.endpoint }}"
        application_name_attributes:
        - "k8s.namespace.name"
        - "service.namespace"
        subsystem_name_attributes:
        - "k8s.deployment.name"
        - "k8s.statefulset.name"
        - "k8s.daemonset.name"
        - "k8s.cronjob.name"
        - "service.name"
        application_name: "{{.Values.global.defaultApplicationName }}"
        subsystem_name: "{{.Values.global.defaultSubsystemName }}"
    processors:
      k8sattributes:
        filter:
          node_from_env_var: KUBE_NODE_NAME
        extract:
          metadata:
            - "k8s.namespace.name"
            - "k8s.deployment.name"
            - "k8s.statefulset.name"
            - "k8s.daemonset.name"
            - "k8s.cronjob.name"
            - "k8s.job.name"
            - "k8s.pod.name"
            - "k8s.node.name"
      # Will get the k8s resource limits
      memory_limiter: null
      resourcedetection/env:
        detectors: ["system", "env"]
        timeout: 2s
        override: false
      spanmetrics:
        metrics_exporter: coralogix
        dimensions:
          - name: "k8s.deployment.name"
          - name: "k8s.statefulset.name"
          - name: "k8s.daemonset.name"
          - name: "k8s.cronjob.name"
          - name: "k8s.job.name"
          - name: "k8s.container.name"
          - name: "k8s.node.name"
          - name: "k8s.namespace.name"
    receivers:
      otlp:
        protocols:
          grpc:
            endpoint: ${MY_POD_IP}:4317
          http:
            endpoint: ${MY_POD_IP}:4318
      zipkin:
        endpoint: ${MY_POD_IP}:9411
      jaeger:
        protocols:
          grpc:
            endpoint: ${MY_POD_IP}:14250
          thrift_http:
            endpoint: ${MY_POD_IP}:14268
          thrift_compact:
            endpoint: ${MY_POD_IP}:6831
          thrift_binary:
            endpoint: ${MY_POD_IP}:6832
      prometheus:
        config:
          scrape_configs:
            - job_name: opentelemetry-collector
              scrape_interval: 30s
              static_configs:
                - targets:
                    - ${MY_POD_IP}:8888
    service:
      extensions:
      - zpages
      - pprof
      - health_check
      - memory_ballast
      telemetry:
        logs:
          encoding: json
        metrics:
          address: ${MY_POD_IP}:8888
      pipelines:
        traces:
          exporters:
            - coralogix
          processors:
            - memory_limiter
            - spanmetrics
            - batch
          receivers:
            - otlp
            - zipkin
            - jaeger
        metrics:
          exporters:
            - coralogix
          processors:
            - memory_limiter
            - resourcedetection/env
            - batch
          receivers:
            - prometheus
            - otlp
        logs:
          exporters:
            - coralogix
          processors:
            - batch
          receivers:
            - otlp
  tolerations:
    - operator: Exists

  resources:
    requests:
      cpu: 100m
      memory: 128Mi
    limits:
      cpu: 1
      memory: 2G

  ports:
    jaeger-binary:
      enabled: true
      containerPort: 6832
      servicePort: 6832
      hostPort: 6832
      protocol: TCP
    # In order to enable podMonitor, following part must be enabled in order to expose the required port:
    # metrics:
    #   enabled: true

  # podMonitor:
  #   enabled: true

  # prometheusRule:
  #   enabled: true
  #   defaultRules:
  #     enabled: true
